\label{sec:classification}
Nachdem in vorherigen Abschnitt eine passende Merkmalsmenge erstellt wurde, geht es nun um die Entscheidung, ob der Fahrer Müde oder Wach ist bzw. ob das System eine Müdigkeitsmeldung erscheinen lässt. Für diese Klassifizierung werden im allgemeinen Machine-Learning-Algorithmen verwendet. Anhand von markierten Datensätzen wird versucht den Algorithmus zu Trainieren (Überwachtes Lernen). Dies dient dem Ziel, dass er auch unbekannte Daten klassifizieren kann. Dieser Vorgang wird Generalisierung bezeichnet und ist auch im menschlichen Lernen ein wichtiger Schritt. Für die Anwendung wurde zur Klassifizierung ein künstliches Neuronales Netz (KNN) ausgewählt. Es basiert auf einem erweiterten Perceptron / McCulloch-Pitts-Neuron \cite{ann} und ist der Funktionsweise des menschlichen Gehirns bzw. seinen Neuronen nachempfunden\cite{marsland_opac-b1129336}. \ann

Die Merkmalsvektoren kommen zu gleichen Teilen aus der Wach und Müde-Menge und sind mit den jeweiligen Klassen 0 und 1 versehen. Insgesamt wurden 1170 Datensätze mit jeweils 24 Werten pro Vektor eingesetzt. Vor dem Training wurde die Merkmalsmenge in Trainings- und Testmenge (2:1) aufgeteilt. Intern wird beim Training noch einmal ein Teil der Daten zum validieren genutzt (15\%). So wird sicher gestellt, dass die Tests nie mit Daten durchgeführt werden, die schon beim Training genutzt wurden. Sonst kann es zum sog. Overfitting kommen, das bedeutet, dass das Netz zu genau auf die Trainingsdaten eingestellt ist und nicht mehr generalisiert.

Für die Anwendung wurde das Training des KNN mit verschiedenen Parametern durchgeführt. Die Menge der Eingabevektoren (X) ist durch die Größe des Merkmalsvektors vorgegeben. Da lediglich 2 Klassen (Wach und Müde) existieren, reicht ein binärer Ausgabevektor aus. Die Frage nach der Menge der Hidden Layer lässt sich nicht per se beantworten. In Versuchen ergaben 4 Schichten die besten Ergebnisse. Weitere Parameter können beim Training eingestellt werden, ein variabler Trägheitsterm (Momentum) verhindert bspw. das Feststecken in lokalen Minima, die Lernrate steuert die Lerngeschwindigkeit im Vergleich zur Genauigkeit. Gute Ergebnisse ließen sich mit einem Momentum 0,25 und einer Lernrate von 0.005 erzielen. Das Training ist beendet, wenn die Gewichte und die Fehlerrate des KNN konvergieren (das kann theoretisch nie passieren) oder die maximale Iterationen durchgeführt wurden. Für das Training wurde eine maximale Iterationszahl von 5000 eingestellt, somit dauerte ein Training ca. 30min. Höhere Iterationszahlen führten nicht zu einer Verbesserung der Ergebnisse. Das Ergebnis ist aufgrund der zufälligen Anfangsparameter immer unterschiedlich, so wurde das Training vier mal parallel in verschiedenen Threads durchgeführt. Bis zum aktuellen Ergebnis wurden ca. 200 Trainingsläufe gestartet.

Die Testmenge wird nun in das KNN eingegeben und überprüft, ob die richtige Klasse erkannt wird. Daraus ergibt sich eine Ergebnis-Matrix (Tab. \ref{tab:ann_results}), in der jede richtig und falsch erkannte Klassifizierung ersichtlich wird. Die Diagonale zeigt die richtig erkannten Klassen und in der letzten Spalte lässt sich die Erkennungsrate pro Klasse erkennen. Da beide Klasse zu gleichen Teilen vorhanden sind, ist die  Gesamt-Erkennungsrate das Mittel der beiden Einzelraten.

\begin{table}[t]
 \centering
 \caption[KNN Ergebnis-Matrix]{Die Tabelle zeigt das Klassifizierungsergebnis Test- und Trainingsdaten \label{tab:ann_results}}
 \begin{tabular}{l|lll}
   & Wach & Müde & Erkannt \\ \hline
  Wach & 357 & 227 & 61\%\\  
  Müde & 217 & 367 & 62\%\\ 
  Gesamt & - & - & 61.5\%\\ 
 \end{tabular}
\end{table}